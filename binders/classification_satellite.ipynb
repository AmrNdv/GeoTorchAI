{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437134cc",
   "metadata": {},
   "source": [
    "# This Example shows how to classify EuroSAT satellite images using the deep learning model DeepSAT-V2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d373ce",
   "metadata": {},
   "source": [
    "Find the details of the DeepSAT-V2 model in the <a href=\"https://arxiv.org/abs/1911.07747\">corresponding paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb9ef5",
   "metadata": {},
   "source": [
    "### EuroSAT satellite dataset contains images from 10 different classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943d9c3",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Annual Crop</li>\n",
    "<li>Forest</li>\n",
    "<li>Herbaceous Vegetation</li>\n",
    "<li>Highway</li>\n",
    "<li>Industrial</li>\n",
    "<li>Pasture</li>\n",
    "<li>Permanent Crop</li>\n",
    "<li>Residential</li>\n",
    "<li>River</li>\n",
    "<li>SeaLake</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea966df",
   "metadata": {},
   "source": [
    "### 13 Spectral bands of a highway image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8f45b",
   "metadata": {},
   "source": [
    "<img src=\"sample-figure/euro-highway.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76e649",
   "metadata": {},
   "source": [
    "### 13 Spectral bands of an industry image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036104e",
   "metadata": {},
   "source": [
    "<img src=\"sample-figure/euro-industry.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from geotorch.models.raster import DeepSatV2\n",
    "from geotorch.datasets.raster import EuroSAT\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters\n",
    "epoch_nums = 100\n",
    "learning_rate = 0.0002\n",
    "batch_size = 16\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = int(time.time())\n",
    "params = {'batch_size': batch_size, 'shuffle': False}\n",
    "\n",
    "## make sure that PATH_TO_DATASET exists in the running directory\n",
    "PATH_TO_DATASET = \"data/eurosat\"\n",
    "MODEL_SAVE_PATH = \"model-deepsatv2\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f83094",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data and calculate mean and std to perform normalization transform\n",
    "## Set download=True if dataset is not available in the given path\n",
    "fullData = EuroSAT(root = PATH_TO_DATASET, download=False)\n",
    "\n",
    "full_loader = DataLoader(fullData, batch_size= batch_size)\n",
    "channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "for i, sample in enumerate(full_loader):\n",
    "    data_temp, _ = sample\n",
    "    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])\n",
    "    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])\n",
    "    num_batches += 1\n",
    "\n",
    "mean = channels_sum / num_batches\n",
    "std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transform operation\n",
    "sat_transform = transforms.Normalize(mean, std)\n",
    "## Load data with desired transformation and additional handcrafted features enabled\n",
    "fullData = EuroSAT(root = PATH_TO_DATASET, include_additional_features = True, transform = sat_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all class names and corresponding labels\n",
    "print(fullData.get_class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff35eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display various bands from an input image\n",
    "input_data, label, features = fullData[1]\n",
    "f, ((ax1, ax2, ax3, ax4, ax5, ax6, ax7), (ax8, ax9, ax10, ax11, ax12, ax13, ax14)) = plt.subplots(2, 7, figsize=(15,5))\n",
    "ax14.axis('off')\n",
    "        \n",
    "ax1.set_title('Band-1')\n",
    "ax1.imshow(input_data[0])\n",
    "\n",
    "ax2.set_title('Band-2')\n",
    "ax2.imshow(input_data[1])\n",
    "\n",
    "ax3.set_title('Band-3')\n",
    "ax3.imshow(input_data[2])\n",
    "\n",
    "ax4.set_title('Band-4')\n",
    "ax4.imshow(input_data[3])\n",
    "\n",
    "ax5.set_title('Band-5')\n",
    "ax5.imshow(input_data[4])\n",
    "\n",
    "ax6.set_title('Band-6')\n",
    "ax6.imshow(input_data[5])\n",
    "\n",
    "ax7.set_title('Band-7')\n",
    "ax7.imshow(input_data[6])\n",
    "\n",
    "ax8.set_title('Band-8')\n",
    "ax8.imshow(input_data[7])\n",
    "\n",
    "ax9.set_title('Band-9')\n",
    "ax9.imshow(input_data[8])\n",
    "\n",
    "ax10.set_title('Band-10')\n",
    "ax10.imshow(input_data[9])\n",
    "\n",
    "ax11.set_title('Band-11')\n",
    "ax11.imshow(input_data[10])\n",
    "\n",
    "ax12.set_title('Band-12')\n",
    "ax12.imshow(input_data[11])\n",
    "\n",
    "ax13.set_title('Band-13')\n",
    "ax13.imshow(input_data[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7799c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize training and validation indices to split the dataset\n",
    "dataset_size = len(fullData)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define training and validation data sampler\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "## Define training and validation data loader\n",
    "train_loader = DataLoader(fullData, **params, sampler=train_sampler)\n",
    "val_loader = DataLoader(fullData, **params, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set device to CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "## Define Model\n",
    "model = DeepSatV2(13, 64, 64, 10, len(fullData.ADDITIONAL_FEATURES))\n",
    "## Define hyper-parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before starting training, define a method to calculate validation accuracy\n",
    "def get_validation_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_sample = 0\n",
    "    correct = 0\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        inputs, labels, features = sample\n",
    "        inputs = inputs.to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs, features)\n",
    "        total_sample += len(labels)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total_sample\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform training and validation\n",
    "max_val_accuracy = None\n",
    "for e in range(epoch_nums):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        inputs, labels, features = sample\n",
    "        inputs = inputs.to(device)\n",
    "        features = features.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, loss.item()))\n",
    "\n",
    "    ## Perform model validation after finishing each epoch training\n",
    "    val_accuracy = get_validation_accuracy(model, val_loader, device)\n",
    "    print(\"Validation Accuracy: \", val_accuracy, \"%\")\n",
    "\n",
    "    if max_val_accuracy == None or val_accuracy > max_val_accuracy:\n",
    "        max_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print('Best model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
